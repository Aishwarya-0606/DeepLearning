{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "App 2 MNIST.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMJppUWxWZ9vJVokmM0hdTL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aish0606/DeepLearning/blob/main/Applications/App_2_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhMLbU05XMMQ"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torchvision.datasets as datasets\r\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ew5SdxIga7QT"
      },
      "source": [
        "# Since we know that the size of input image is 28x28 so input features= 28x28 = 784\r\n",
        "input_size = 784     #Number of input neurons (image pixels)\r\n",
        "hidden_size = 400    # hidden_neuron=input_neuron+output_neuron/2 (approx.)\r\n",
        "output_size = 10     #Number of classes (0-9)\r\n",
        "epochs = 10          #How many times we pass our entire dataset into our network \r\n",
        "batch_size = 100       #Input size of the data during one iteration\r\n",
        "learning_rate = 0.001   #How fast we are learning"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9wns-e6DOHO"
      },
      "source": [
        "train_data = datasets.MNIST(root='./data',\r\n",
        "                            train=True,\r\n",
        "                            transform=transforms.ToTensor(),\r\n",
        "                            download=True)\r\n",
        "test_data = datasets.MNIST(root='./data',\r\n",
        "                           train=False,\r\n",
        "                           transform=transforms.ToTensor())"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fFi5IekEZze"
      },
      "source": [
        "# When we have to load the data in batches, we use dataloader\r\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_data,\r\n",
        "                                           batch_size=batch_size,\r\n",
        "                                           shuffle=True)\r\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_data,\r\n",
        "                                          batch_size=batch_size,\r\n",
        "                                          shuffle=True)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQlC8BsB-kpg",
        "outputId": "89622701-e1f3-4f8c-b175-4d77a4bfd224"
      },
      "source": [
        "print(len(train_data))\r\n",
        "print(len(train_loader))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000\n",
            "600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fg7mEJX1IGCB"
      },
      "source": [
        "class Net(nn.Module):\r\n",
        "  def __init__(self, input_size, hidden_size, output_size):\r\n",
        "    super(Net, self).__init__()\r\n",
        "    self.fc1 = nn.Linear(input_size, hidden_size)\r\n",
        "    self.fc2 = nn.Linear(hidden_size, hidden_size) # Here we have 2 hidden layers\r\n",
        "    self.fc3 = nn.Linear(hidden_size, output_size)\r\n",
        "    self.relu = nn.ReLU()\r\n",
        "    self.init_weight()\r\n",
        "  \r\n",
        "  def init_weight(self):\r\n",
        "    nn.init.kaiming_normal_(self.fc1.weight)\r\n",
        "    nn.init.kaiming_normal_(self.fc2.weight)\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    out = self.fc1(x)\r\n",
        "    out = self.relu(out)\r\n",
        "    out = self.fc2(out)\r\n",
        "    out = self.relu(out)\r\n",
        "    out = self.fc3(out)\r\n",
        "    return out"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9N7C-yfWqlpJ"
      },
      "source": [
        "# Create an object of the class\r\n",
        "net = Net(input_size=input_size, hidden_size=hidden_size, output_size=output_size)\r\n",
        "CUDA = torch.cuda.is_available()\r\n",
        "if CUDA:\r\n",
        "  net = net.cuda()\r\n",
        "#The loss function. The Cross Entropy loss comes along with Softmax. \r\n",
        "#Therefore, no need to specify Softmax as well\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oz1feU-0qs62",
        "outputId": "d6942d49-3c5b-4a40-c94d-4d27ec5f7bff"
      },
      "source": [
        "# What is in net.parameters\r\n",
        "print(len(list(net.parameters())))\r\n",
        "print(net.parameters)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6\n",
            "<bound method Module.parameters of Net(\n",
            "  (fc1): Linear(in_features=784, out_features=400, bias=True)\n",
            "  (fc2): Linear(in_features=400, out_features=400, bias=True)\n",
            "  (fc3): Linear(in_features=400, out_features=10, bias=True)\n",
            "  (relu): ReLU()\n",
            ")>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UidHswUr0v1"
      },
      "source": [
        "# Visualize the train_loader\r\n",
        "for i, (images, labels) in enumerate(train_loader):\r\n",
        "  #print(images.size())\r\n",
        "  # changing the dimension of tensors from 4D to 2D.\r\n",
        "  images = images.view(-1, 784)\r\n",
        "  #print(images.size())"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WypE0x5VCsAf"
      },
      "source": [
        "# print(labels.shape)\r\n",
        "# labels = labels.unsqueeze(1)\r\n",
        "# print(labels.shape)\r\n",
        "# print(labels.dtype)\r\n",
        "# print(predicted.dtype)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkeUN5d20x2Y",
        "outputId": "d1a7dcce-1bd3-4789-af44-39ca1db97338"
      },
      "source": [
        "# train the network\r\n",
        "for epoch in range(epochs):\r\n",
        "  correct_train = 0\r\n",
        "  running_loss = 0\r\n",
        "  for i, (images, labels) in enumerate(train_loader):\r\n",
        "    #Flatten the image from size (batch,1,28,28) --> (100,1,28,28) \r\n",
        "    #where 1 represents the number of channels (grayscale-->1),\r\n",
        "    # to size (100,784)\r\n",
        "    images = images.view(-1, 784)\r\n",
        "    if CUDA:\r\n",
        "      images = images.cuda()\r\n",
        "      labels = labels.cuda()\r\n",
        "    outputs = net(images)\r\n",
        "    _, predicted = torch.max(outputs.data, 1)  # returns max_value & index of max_val\r\n",
        "\r\n",
        "    correct_train += (labels == predicted).sum()\r\n",
        "    #predicted = predicted.unsqueeze(1)\r\n",
        "    loss = criterion(outputs, labels)\r\n",
        "    running_loss += loss.item()\r\n",
        "    # Clear the param_grad so it won't be accumulated\r\n",
        "    optimizer.zero_grad()\r\n",
        "    loss.backward()    #Backpropogation\r\n",
        "    optimizer.step()   # Weight updation\r\n",
        "\r\n",
        "  print('Epoch: {} / {}. Training Loss: {}. Training Accuracy: {}'.format(\r\n",
        "      epoch, epochs, running_loss/len(train_loader), 100 * correct_train/len(train_data)))\r\n",
        "\r\n",
        "print(\"TRAINING DONE\")"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 / 10. Training Loss: 0.2383550605022659. Training Accuracy: 93.03500366210938\n",
            "Epoch: 1 / 10. Training Loss: 0.08361979553941637. Training Accuracy: 97.42833709716797\n",
            "Epoch: 2 / 10. Training Loss: 0.057043617534606406. Training Accuracy: 98.22833251953125\n",
            "Epoch: 3 / 10. Training Loss: 0.03952181260644769. Training Accuracy: 98.74166870117188\n",
            "Epoch: 4 / 10. Training Loss: 0.031252037803642455. Training Accuracy: 98.95000457763672\n",
            "Epoch: 5 / 10. Training Loss: 0.0216145265243055. Training Accuracy: 99.28166961669922\n",
            "Epoch: 6 / 10. Training Loss: 0.020103458532394144. Training Accuracy: 99.34166717529297\n",
            "Epoch: 7 / 10. Training Loss: 0.01715963010124445. Training Accuracy: 99.40166473388672\n",
            "Epoch: 8 / 10. Training Loss: 0.014033533431841837. Training Accuracy: 99.53666687011719\n",
            "Epoch: 9 / 10. Training Loss: 0.015138558352373366. Training Accuracy: 99.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BLseNb7-y0x",
        "outputId": "b5f4b20f-682a-48bb-9dc6-ff0a3f087812"
      },
      "source": [
        "print(labels)\r\n",
        "print(len(labels))\r\n",
        "print(predicted)\r\n",
        "print(len(outputs))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0, 9, 1, 1, 7, 6, 7, 1, 9, 3, 7, 8, 8, 0, 9, 1, 3, 8, 3, 8, 4, 4, 5, 9,\n",
            "        9, 8, 0, 7, 2, 1, 1, 4, 0, 0, 6, 7, 0, 3, 4, 8, 8, 2, 6, 6, 1, 7, 0, 0,\n",
            "        1, 9, 8, 0, 4, 7, 2, 9, 0, 3, 9, 1, 9, 9, 6, 4, 0, 7, 9, 3, 4, 5, 3, 1,\n",
            "        6, 5, 3, 4, 8, 5, 7, 8, 8, 7, 9, 3, 8, 7, 5, 8, 4, 6, 4, 4, 7, 8, 6, 1,\n",
            "        3, 1, 3, 9], device='cuda:0')\n",
            "100\n",
            "tensor([0, 9, 1, 1, 7, 6, 7, 1, 9, 3, 7, 8, 8, 0, 9, 1, 3, 8, 3, 8, 4, 4, 5, 9,\n",
            "        9, 8, 0, 7, 2, 1, 1, 4, 0, 0, 6, 7, 0, 3, 4, 1, 8, 2, 6, 6, 1, 7, 0, 0,\n",
            "        1, 9, 8, 0, 4, 7, 2, 9, 0, 3, 9, 1, 9, 9, 6, 4, 0, 7, 9, 3, 4, 5, 3, 1,\n",
            "        6, 5, 3, 4, 8, 5, 7, 8, 8, 7, 9, 3, 8, 7, 5, 8, 4, 6, 4, 4, 7, 8, 6, 1,\n",
            "        3, 1, 3, 9], device='cuda:0')\n",
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3n5OaQh7GYGE",
        "outputId": "838345bc-f73e-4081-9fea-a88cc77f9805"
      },
      "source": [
        "# Testing \r\n",
        "with torch.no_grad():\r\n",
        "  correct = 0\r\n",
        "  for images, labels in test_loader:\r\n",
        "    if CUDA:\r\n",
        "      images = images.cuda()\r\n",
        "      labels = labels.cuda()\r\n",
        "    images = images.view(-1, 784) # 28*28 = 784\r\n",
        "    outputs = net(images)\r\n",
        "    _, predicted = torch.max(outputs, 1)\r\n",
        "    correct += (predicted == labels).sum()\r\n",
        "  \r\n",
        "  print('Accuracy for 10000 test images are {}'.format(100 * (correct / len(test_data))))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy for 10000 test images are 98.20999145507812\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc9AK45_Pi8k"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}